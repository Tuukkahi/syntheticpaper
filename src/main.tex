\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[numbers]{natbib}
\usepackage{geometry}
\geometry{margin=1in}

\title{Synthetic data trained neural network}
\author{}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\begin{itemize}
\item
    Satellite data with missing values or incomplete data. Retrospective reconstruction of time series. The task is to make forecasts or complete inclomplete historical data.

    \item
        Physics-constrained neural network
    \item
        Advection-(diffusion) model of dynamical systems.

        Assume simple advection diffusion dynamics. Solved numerically by (semi) Lagrangian schema. (aka warping). Using torch.grid.sample2d with nn interpolation. Some example of "warping".

        How about diffusion. Now it is not explicitly in the model. There is evident numerical diffusion, or is there? At least dimension reduction filter smooths the state. Kalman filter has spatially correlated model error that smooths the results, too.

        We do not model diffusion and in the COD example, the clouds do not diffuse(?). However, we might add small diffusion temp to the model in order to hide artefacts coming from numerics(?)

    \item
        Synthetic training data simulation

        Wee need data for training the model. One possibility is to simulate realistic data. Then we are not constrained by the amount of training data. But it is not clear  how well these model will perform.

    \item
        Dimension independence

        Advection fields are parameterized using 2d polynomial (or splines, etc). This makes the recontruction independent of the domain and discretization.

    \item
        Short-term prediction

        A practical application would be short term forecast, i.e., predicting the dynamics of the system a couple of time steps ahead. For the COD example, these could be used to predict solar energy production in the few coming hours. Similar models are used in many meteorological applications, for example short term rain forecasting using weather radar images.

    \item
        Spatio-temporal interpolation

        Interpolation of historical data to construct homogenized time series.

    \item
        Dimension reduction Kalman filter for posterior uncertainty

        Efficient algorithms utilizing PyTorch.

    \item Some example using COD data. Maybe some simple "baby" example first?

\end{itemize}







\section{Materials and methods}
\subsection{Advection model}
Advection equation on the plane
\begin{equation}\label{eq:adv}
    \frac{\partial u}{\partial t} = F \cdot \nabla u
\end{equation}
$u: \mathbb{R}^2 \times [0, \infty)  \rightarrow \mathbb{R}$, $F: \mathbb{R}^2 \times [0, \infty) \rightarrow \mathbb{R}^2$
For temporally sparse gridded observations, we employ a semi-Lagrangian scheme to solve the equation numerically.
The semi-Lagrangian discretized form for the advection equation~\eqref{eq:adv} is
\begin{equation}
    \frac{u_h(x, t^{n+1}) - u_h(\tilde{x}, t^n)}{\Delta t} = 0
\end{equation}
where $x \in [0, \dots, n_1] \times [0, \dots, n_2]$ is the index set of $u_h$, $\tilde{x}$ the estimated departure index of $u_h(x, t^{n+1})$, and $\Delta t$ the time step.
For variable vector field $F_h$ of the advection, the index of departure is then given by
\begin{equation}
    \tilde{x} = x - F_h(x, t) \Delta t.
\end{equation}
The interpolation of the departure index plays a crucial role in the numerical accuracy of the method.

Because the semi-Lagrangian discretization method is fundamentally a mere re-indexing operation, it is quick to compute even in large scales when $F_h$ is known.
Moreover, this avoids unwanted numerical diffusion that would otherwise originate from computing the explicit gradients in larger time steps and is therefore, often used in atmospheric modelling~\cite{diamantakis}.

\subsection{Neural network model}
The semi-Lagrangian scheme provides a fast time integration method for our problem but the vector field is required for the solution.
Neural networks have previously been used to predict pixel-wise values for full vector fields with the training objective being ground-truth vector fields~\cite{ofnn}, or future images~\cite{debezenac}.
In high-noise real-world applications where the underlying vector fields can be assumed to take some general form, these methods output noisy estimates of vector fields and thereby limiting their usefulness.
To rectify this issue, we parametrize the vector field using a low-dimensional basis, in this case the space of second degree polynomials on the plane
\begin{equation}
    \mathbb{P}_2(\mathbb{R}^2, \mathbb{R}^2) =
    \left\{
        \begin{bmatrix}
            a_1 + a_2x + a_3y + a_4xy + a_5x^2 + a_6y^2\\
            b_1 + b_2x + b_3y + b_4xy + b_5x^2 + b_6y^2
        \end{bmatrix} \,
    \middle| \,
        a_i, b_i \in \mathbb{R}
    \right\}
\end{equation}
In addition to noise robustness, the basis function parametrization solves a number of typical challenges.

With the selection of the basis, the complexity of the predicted vector fields can be easily controlled depending on the application.
The model is also, in principle, dimension independent as the inputs can be interpolated to the resolution at which the model was trained, and the output polynomial can then be evaluated on the original input grid to then use the semi-Lagrangian scheme to do the time integration.
With the basis function parametrization the vector fields can be represented by only a handful of parameters.
Thus we can also re-use a standard convolutional image recognition neural network easily as opposed to more complex and resource intensive architectures.

For predicting the vector fields, we use a standard 28-layer convolutional ResNet neural network~\cite{resnet}.

\subsection{Synthetic training data generation scheme}
In the absence of real-world datasets with ground-truth vector fields, we rely on synthetic training data to train our model.
By simulating data with a wide range of spatial structures, noise levels, and flow patterns, we aim at training a model that generalises beyond any specific data source.
Each synthetic sample is generated on demand during training, eliminating need for large dataset storage and providing a virtually unlimited supply of training data.

The synthetic data for each training sample is generated as follows:
\begin{enumerate}
    \item 
        \textbf{Initial Field Sampling}

        Initially we sample a random $\mathbb{R}^2$ field with the Matérn covariance
        \begin{equation}
            C_{\nu}(u,v) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu}}{\ell} \lVert u - v \rVert\right)^{\nu} K_{\nu}\left(\frac{\sqrt{2\nu}}{\ell} \lVert u - v \rVert \right)
        \end{equation}
        from the Matérn-Whittle stochastic partial differential equation (SPDE)~\cite{whittle63, lindgren}
        \begin{equation}
            (\kappa^2 - \Delta)^{\frac{\alpha}{2}} X(u) = W(u), \quad W \sim \mathcal{N}(0, I).
        \end{equation}
        Samples can be then drawn by evaluating the inverse Fourier transform
        \begin{equation}
            X(u) = \frac{1}{(2 \pi)^2} \int_{\mathbb{R}^2} \frac{e^{-i u \cdot \eta} \hat{W}(\xi)}{\kappa^2 - \Vert \xi \Vert ^{\frac{\alpha}{2}}} \, d \xi
        \end{equation}
        for different realisations of white noise.

    \item 
        \textbf{Vector Field Sampling}

        A vector field is generated by sampling an integer magnitude within a specified range for each component of vectors at interpolation nodes.
        This is followed by a random global sign for each component to ensure diversity in all directions.
        The quadratic polynomial vector field is then solved, as descibed in~\ref{???}.
        These vector fields serves as the training objective for the model.

    \item
        \textbf{Numerical Advection and Noise Injection}

        The initial field is advected according to the generated vector field, creating a temporal sequence of images.
        At each step, small random integer perturbations are added to the vector fields to mimic dynamic variability of the vector fields and to show the network training data where advection is not strictly defined by quadratic polynomial vector fields.
        The target vector field in training remains the unperturbed quadratic one.

        In addition, at each step additive spatially correlated noise with sampled similarly as the initial field but with a random amplitude drawn between $[\sigma_{\text{min}}, \sigma_{\text{max}}]$.
        This exposes the neural network to a broad range of signal-to-noise ratios to enhances its robustness for real-world data.
\end{enumerate}
\subsection{Dimension reduction Kalman filter}
\cite{solonen}.

\section{Numerical experiments}

\subsection{COD data}


\section{Discussion}


\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
